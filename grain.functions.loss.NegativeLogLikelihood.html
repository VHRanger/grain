<!DOCTYPE html>
<html>
<head>
    <title>NegativeLogLikelihood (grain.functions.loss.NegativeLogLikelihood)</title>
    <meta charset="utf-8" />
    <meta content="width=device-width, initial-scale=1" name="viewport" />
        <link rel="stylesheet" href="style.css" />
        <script type="text/javascript" src="script.js"></script>

    
    <link rel="prefetch" href="search-results.html" />
</head>
<body>
    <div id="page-header">
        <div id="logotype">
        <span>Documentation</span>
        <nav>
            <a href="https://github.com/ShigekiKarita/grain">GitHub</a>
            <a href="https://github.com/ShigekiKarita/grain/tree/master/example">Example</a>
            <a href="http://dlang.org/">Dlang.org</a>
        </nav>
        </div>

        <form action="search-docs.html" id="search">
            <input type="search" placeholder="Find a symbol name..." name="searchTerm" />
            <input type="submit" value="Go" />
        </form>
    </div>
    <div id="page-body">
        <div id="page-content">
        <h1>NegativeLogLikelihood</h1><div class="breadcrumbs"><a href="grain.html" class="breadcrumb">grain</a><a href="grain.functions.html" class="breadcrumb">functions</a><a href="grain.functions.loss.html" class="breadcrumb">loss</a></div><div><div class="documentation-comment synopsis"><div></div></div></div><div class="annotated-prototype"><div class="aggregate-prototype"><div class="attributes"></div><span class="builtin-type">struct</span> NegativeLogLikelihood (<div class="parameters-list toplevel"><div class="template-parameter-item parameter-item">	<span><span data-ident="F" class="name">F</span></span></div><div class="template-parameter-item parameter-item">	<span><span data-ident="I" class="name">I</span> = <span class="builtin-type">long</span></span></div></div>) {<div class="aggregate-member"><a href="grain.functions.loss.NegativeLogLikelihood.__anonymous.html"><tt class="highlighted"><span class="kwrd">mixin</span> <span class="hid">FunctionCommon</span> </tt>;</a></div><div class="aggregate-member"><tt class="highlighted"><span class="type">bool</span></tt> <span class="name">sizeAverage</span>;</div><div class="aggregate-member"><tt class="highlighted"><span class="type">int</span></tt> <span class="name">ignoreIndex</span>;</div><div class="aggregate-member"><tt class="highlighted"><span class="hid">Variable</span>!(<span class="hid">I</span>, <span class="num">1</span>, <span class="hid">HostStorage</span>)</tt> <span class="name">_htargetId</span>;</div><div class="aggregate-member"><tt class="highlighted"><span class="hid">F</span></tt> <span class="name">_normalize</span>;</div><div class="aggregate-member"><tt class="highlighted"><span class="type">int</span></tt> <span class="name">_nClass</span>;</div><div class="aggregate-member"><tt class="highlighted"><span class="hid">Variable</span>!(<span class="hid">I</span>, <span class="num">1</span>, <span class="hid">DeviceStorage</span>)</tt> <span class="name">_dtargetId</span>;</div>}</div></div><h2 id="members"><a href="#members" class="header-anchor">Members</a></h2><h3 id="mixin" class="member-list-header hide-from-toc"><a href="#mixin" class="header-anchor">Mixins</a></h3><dl class="member-list native"><dt><a href="grain.functions.loss.NegativeLogLikelihood.__anonymous.html">__anonymous</a><div style="max-width: 23ch;" class="simplified-prototype"><tt class="highlighted"><span class="kwrd">mixin</span> <span class="hid">FunctionCommon</span> </tt></div></dt><dd><div><p>Compute negative log-likelihood: -logP(y=t)</p></div></dd></dl><div><h2 id="examples"><a href="#examples" class="header-anchor">Examples</a></h2><div class="documentation-comment"><div></div></div><div class="unittest-example-holder"><div class="documentation-comment"><p>test nll simple case, gradcheck and cpu/cuda equality</p></div><pre class="d_code highlighted with-line-wrappers"><span class="br">1 </span><span class="com">/++ equivalent torch v0.4 code
<span class="br">2 </span> &gt;&gt;&gt; x = torch.FloatTensor([[0.2, 0.4, 0.4], [0.1,0.5,0.4]])
<span class="br">3 </span> &gt;&gt;&gt; x.requires_grad = True
<span class="br">4 </span> &gt;&gt;&gt; t = torch.LongTensor([1, 0])
<span class="br">5 </span> &gt;&gt;&gt; l = torch.nn.functional.nll_loss(x, t)
<span class="br">6 </span> &gt;&gt;&gt; print(l)
<span class="br">7 </span> tensor(-0.2500)
<span class="br">8 </span>
<span class="br">9 </span> &gt;&gt;&gt; l.backward()
<span class="br">10 </span> &gt;&gt;&gt; print(x.grad)
<span class="br">11 </span> tensor([[0.0, -0.5, 0.0], [-0.5, 0.0, 0.0]])
<span class="br">12 </span> +/</span>
<span class="br">13 </span><span class="kwrd">import</span> <span class="hid">std</span>.<span class="hid">typecons</span>;
<span class="br">14 </span><span class="kwrd">import</span> <span class="hid">grain</span>.<span class="hid">testing</span>;
<span class="br">15 </span>
<span class="br">16 </span><span class="hid">NegativeLogLikelihood</span>!(<span class="type">float</span>, <span class="type">int</span>) <span class="hid">func</span>;
<span class="br">17 </span><span class="kwrd">auto</span> <span class="hid">hx</span> = [[<span class="num">0.2f</span>, <span class="num">0.4f</span>, <span class="num">0.4f</span>], [<span class="num">0.1f</span>, <span class="num">0.5f</span>, <span class="num">0.4f</span>], [<span class="num">0.1f</span>, <span class="num">0.5f</span>, <span class="num">0.4f</span>]]
<span class="br">18 </span>    .<span class="hid">variable</span>;
<span class="br">19 </span><span class="kwrd">auto</span> <span class="hid">ht</span> = [<span class="num">1</span>, <span class="num">0</span>, <span class="hid">func</span>.<span class="hid">ignoreIndex</span>].<span class="hid">variable</span>;
<span class="br">20 </span><span class="kwrd">auto</span> <span class="hid">hl</span> = <span class="hid">func</span>.<span class="hid">forward</span>(<span class="hid">hx</span>, <span class="hid">ht</span>);
<span class="br">21 </span><span class="kwrd">assert</span>(<span class="hid">func</span>.<span class="hid">_normalize</span> == <span class="num">0.5</span>);
<span class="br">22 </span><span class="kwrd">assert</span>(<span class="hid">hl</span>.<span class="hid">sliced</span> == [-(<span class="num">0.4f</span> + <span class="num">0.1f</span> + <span class="num">0.0f</span>) / <span class="num">2</span>]);
<span class="br">23 </span><span class="kwrd">auto</span> <span class="hid">hgx</span> = <span class="hid">func</span>.<span class="hid">backward</span>(<span class="num">1.0f</span>.<span class="hid">variable</span>);
<span class="br">24 </span><span class="kwrd">assert</span>(<span class="hid">hgx</span>[<span class="num">0</span>].<span class="hid">sliced</span> == [[<span class="num">0.0</span>, -<span class="num">0.5</span>, <span class="num">0.0</span>], [-<span class="num">0.5</span>, <span class="num">0.0</span>, <span class="num">0.0</span>], [<span class="num">0.0</span>, <span class="num">0.0</span>, <span class="num">0.0</span>]]);
<span class="br">25 </span><span class="kwrd">assert</span>(!<span class="hid">hgx</span>[<span class="num">1</span>].<span class="hid">defined</span>);
<span class="br">26 </span><span class="hid">gradCheck</span>(<span class="hid">func</span>, <span class="hid">tuple</span>(<span class="hid">hx</span>, <span class="hid">ht</span>), <span class="num">1.0f</span>.<span class="hid">variable</span>);
<span class="br">27 </span>
<span class="br">28 </span><span class="kwrd">version</span> (<span class="hid">grain_cuda</span>) {
<span class="br">29 </span>    <span class="kwrd">auto</span> <span class="hid">dx</span> = <span class="hid">hx</span>.<span class="hid">to</span>!<span class="hid">DeviceStorage</span>;
<span class="br">30 </span>    <span class="kwrd">auto</span> <span class="hid">dt</span> = <span class="hid">ht</span>.<span class="hid">to</span>!<span class="hid">DeviceStorage</span>;
<span class="br">31 </span>    <span class="kwrd">auto</span> <span class="hid">dl</span> = <span class="hid">func</span>.<span class="hid">forward</span>(<span class="hid">dx</span>, <span class="hid">dt</span>);
<span class="br">32 </span>    <span class="kwrd">assert</span>(<span class="hid">func</span>.<span class="hid">_normalize</span> == <span class="num">0.5</span>);
<span class="br">33 </span>    <span class="kwrd">assert</span>(<span class="hid">dl</span>.<span class="hid">to</span>!<span class="hid">HostStorage</span>.<span class="hid">sliced</span> == [-(<span class="num">0.4f</span> + <span class="num">0.1f</span> + <span class="num">0.0f</span>) / <span class="num">2</span>]);
<span class="br">34 </span>    <span class="kwrd">auto</span> <span class="hid">dgx</span> = <span class="hid">func</span>.<span class="hid">backward</span>(<span class="num">1.0f</span>.<span class="hid">variable</span>.<span class="hid">to</span>!<span class="hid">DeviceStorage</span>);
<span class="br">35 </span>    <span class="kwrd">assert</span>(<span class="hid">dgx</span>[<span class="num">0</span>].<span class="hid">to</span>!<span class="hid">HostStorage</span>.<span class="hid">sliced</span> ==
<span class="br">36 </span>           [[<span class="num">0.0</span>, -<span class="num">0.5</span>, <span class="num">0.0</span>],
<span class="br">37 </span>            [-<span class="num">0.5</span>, <span class="num">0.0</span>, <span class="num">0.0</span>],
<span class="br">38 </span>            [<span class="num">0.0</span>, <span class="num">0.0</span>, <span class="num">0.0</span>]]);
<span class="br">39 </span>    <span class="kwrd">assert</span>(!<span class="hid">dgx</span>[<span class="num">1</span>].<span class="hid">defined</span>);
<span class="br">40 </span>}</pre></div><div class="unittest-example-holder"><div class="documentation-comment"><p>test variable.backward</p></div><pre class="d_code highlighted with-line-wrappers"><span class="br">1 </span><span class="kwrd">import</span> <span class="hid">std</span>.<span class="hid">typecons</span>;
<span class="br">2 </span><span class="kwrd">import</span> <span class="hid">grain</span>.<span class="hid">testing</span>;
<span class="br">3 </span><span class="kwrd">import</span> <span class="hid">mir</span>.<span class="hid">ndslice</span>;
<span class="br">4 </span>
<span class="br">5 </span><span class="hid">grain</span>.<span class="hid">autograd</span>.<span class="hid">backprop</span> = <span class="kwrd">true</span>;
<span class="br">6 </span>
<span class="br">7 </span><span class="hid">NegativeLogLikelihood</span>!(<span class="type">float</span>, <span class="type">int</span>) <span class="hid">func</span>;
<span class="br">8 </span><span class="kwrd">auto</span> <span class="hid">hx</span> = [[<span class="num">0.2f</span>, <span class="num">0.4f</span>, <span class="num">0.4f</span>], [<span class="num">0.1f</span>, <span class="num">0.5f</span>, <span class="num">0.4f</span>], [<span class="num">0.1f</span>, <span class="num">0.5f</span>, <span class="num">0.4f</span>]]
<span class="br">9 </span>    .<span class="hid">variable</span>;
<span class="br">10 </span><span class="hid">hx</span>.<span class="hid">requiresGrad</span> = <span class="kwrd">true</span>;
<span class="br">11 </span><span class="kwrd">auto</span> <span class="hid">ht</span> = [<span class="num">1</span>, <span class="num">0</span>, <span class="hid">func</span>.<span class="hid">ignoreIndex</span>].<span class="hid">variable</span>;
<span class="br">12 </span><span class="kwrd">auto</span> <span class="hid">hl</span> = <span class="hid">func</span>.<span class="hid">applyForward</span>(<span class="hid">hx</span>, <span class="hid">ht</span>);
<span class="br">13 </span>
<span class="br">14 </span><span class="kwrd">assert</span>(<span class="hid">func</span>.<span class="hid">_normalize</span> == <span class="num">0.5</span>);
<span class="br">15 </span><span class="kwrd">assert</span>(<span class="hid">hl</span>.<span class="hid">sliced</span> == [-(<span class="num">0.4f</span> + <span class="num">0.1f</span> + <span class="num">0.0f</span>) / <span class="num">2</span>]);
<span class="br">16 </span><span class="kwrd">auto</span> <span class="hid">u</span> = <span class="hid">UntypedVariable</span>(<span class="num">1.0f</span>.<span class="hid">variable</span>);
<span class="br">17 </span><span class="hid">hl</span>.<span class="hid">backward</span>(&amp;<span class="hid">u</span>);
<span class="br">18 </span>
<span class="br">19 </span><span class="kwrd">assert</span>(<span class="hid">hx</span>.<span class="hid">grad</span>[].<span class="hid">sliced</span>(<span class="num">3</span>, <span class="num">3</span>) == [[<span class="num">0.0</span>, -<span class="num">0.5</span>, <span class="num">0.0</span>], [-<span class="num">0.5</span>, <span class="num">0.0</span>, <span class="num">0.0</span>], [<span class="num">0.0</span>, <span class="num">0.0</span>,
<span class="br">20 </span>        <span class="num">0.0</span>]]);
<span class="br">21 </span><span class="com">// TODO assert(!ht.grad.defined);</span></pre></div><h2 id="meta"><a href="#meta" class="header-anchor">Meta</a></h2><div class="documentation-comment source-section other-section"><h3 id="source"><a href="#source" class="header-anchor">Source</a></h3><div><p><a href="source/grain.functions.loss.d.html#L12">See Implementation</a><br /></p></div></div></div></div>
        <div id="page-nav"><a href="grain.html" class="parent">grain</a><a href="grain.functions.html" class="parent">functions</a><a href="grain.functions.loss.html" class="parent">loss</a>
        <span class="type-separator">structs</span><ul><li><a href="grain.functions.loss.NegativeLogLikelihood.html" class="struct current">NegativeLogLikelihood</a></li></ul></div>
    </div>
    <div id="page-footer">
        Distributed under <a href="https://www.boost.org/users/license.html">BSL-1.0</a>.
        Copyright <a href="https://shigekikarita.github.io">Shigeki Karita</a> 2018.
        Page generated by <a href="https://github.com/adamdruppe/adrdox">adrdox</a>
    </div>
</body>
</html>